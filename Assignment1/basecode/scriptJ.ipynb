{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.io import loadmat\n",
    "from numpy.linalg import det, inv\n",
    "from math import sqrt, pi\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ldaLearn(X,y):\n",
    "    # Inputs\n",
    "    # X - a N x d matrix with each row corresponding to a training example\n",
    "    # y - a N x 1 column vector indicating the labels for each training example\n",
    "    #\n",
    "    # Outputs\n",
    "    # means - A d x k matrix containing learnt means for each of the k classes\n",
    "    # covmat - A single d x d learnt covariance matrix \n",
    "    \n",
    "    # IMPLEMENT THIS METHOD\n",
    "    \n",
    "    class_list = np.unique(y)  # Creating a unique class list with size of 1 * d\n",
    "    \n",
    "    # Create an empty d * k matrix for means\n",
    "    means = np.zeros((X.shape[1], class_list.shape[0]))\n",
    "    \n",
    "    # Put all the mean column vector into means matrix\n",
    "    j = 0\n",
    "    for i in class_list:     # Iterate the class list\n",
    "        location = np.where(y == i)[0]   # Find row indexes for a specific class\n",
    "        mean = np.mean(X[location], axis=0)\n",
    "        means[:, j] = mean.transpose()\n",
    "        j = j + 1\n",
    "\n",
    "    # Calculate the mean of every column for means matrix\n",
    "    ground_mean = np.mean(X, axis=0)\n",
    "\n",
    "    # Calculate the covariance matrix of X\n",
    "    covmat = np.cov((X - ground_mean).transpose())\n",
    "    \n",
    "    return means,covmat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qdaLearn(X,y):\n",
    "    # Inputs\n",
    "    # X - a N x d matrix with each row corresponding to a training example\n",
    "    # y - a N x 1 column vector indicating the labels for each training example\n",
    "    #\n",
    "    # Outputs\n",
    "    # means - A d x k matrix containing learnt means for each of the k classes\n",
    "    # covmats - A list of k d x d learnt covariance matrices for each of the k classes\n",
    "    \n",
    "    # IMPLEMENT THIS METHOD\n",
    "    \n",
    "    class_list = np.unique(y)  # Creating a unique class list\n",
    "\n",
    "    # Create an empty d*k matrix for means. And it is the same as LDA\n",
    "    means = np.zeros((X.shape[1], class_list.shape[0]))\n",
    "\n",
    "    j = 0\n",
    "    covmats = []\n",
    "\n",
    "    for i in class_list.transpose():\n",
    "        location2 = np.where(y == i)[0]\n",
    "        mean2 = np.mean(X[location2], axis=0)\n",
    "        means[:, j] = mean2.transpose()\n",
    "        dif = X[location2, ] - mean2\n",
    "        covmats.append(np.cov(dif.transpose()))\n",
    "        j = j + 1\n",
    "\n",
    "    return means,covmats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ldaTest(means,covmat,Xtest,ytest):\n",
    "    # Inputs\n",
    "    # means, covmat - parameters of the LDA model\n",
    "    # Xtest - a N x d matrix with each row corresponding to a test example\n",
    "    # ytest - a N x 1 column vector indicating the labels for each test example\n",
    "    # Outputs\n",
    "    # acc - A scalar accuracy value\n",
    "    # ypred - N x 1 column vector indicating the predicted labels\n",
    "\n",
    "    # IMPLEMENT THIS METHOD\n",
    "    \n",
    "    # Create a new empty matrix with N*k size\n",
    "    result_p = np.zeros((Xtest.shape[0], means.shape[1]))\n",
    "\n",
    "    # prior probability\n",
    "    p = 1/np.sqrt((2*pi) ** means.shape[0] * det(covmat))\n",
    "\n",
    "    # Calculate probabilities of every Xi for every y\n",
    "    j = 0\n",
    "    for i in means.transpose():\n",
    "        mean = i\n",
    "        Prob = p * np.exp(- 0.5 * np.sum([a * b for a, b in zip(np.dot(Xtest - mean, inv(covmat)), Xtest - mean)], axis=1))\n",
    "        result_p[:, j] = Prob\n",
    "        j = j + 1\n",
    "\n",
    "    ypred = np.argmax(result_p, axis=1) + 1\n",
    "    error = ypred - ytest.ravel()\n",
    "    acc = (Xtest.shape[0] - float(len(np.nonzero(error)[0])))/Xtest.shape[0]\n",
    "\n",
    "    return acc,ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qdaTest(means,covmats,Xtest,ytest):\n",
    "    # Inputs\n",
    "    # means, covmats - parameters of the QDA model\n",
    "    # Xtest - a N x d matrix with each row corresponding to a test example\n",
    "    # ytest - a N x 1 column vector indicating the labels for each test example\n",
    "    # Outputs\n",
    "    # acc - A scalar accuracy value\n",
    "    # ypred - N x 1 column vector indicating the predicted labels\n",
    "\n",
    "    # IMPLEMENT THIS METHOD\n",
    "    \n",
    "    # Create a new empty matrix with N*k size\n",
    "    result_p2 = np.zeros((Xtest.shape[0], means.shape[1]))\n",
    "\n",
    "    j = 0\n",
    "    for i in means.transpose():\n",
    "        p = 1 / np.sqrt((2*pi) ** means.shape[0] * det(covmats[j]))\n",
    "        mean = i\n",
    "\n",
    "        Prob = p * np.exp(- 0.5 * np.sum([a * b for a, b in zip(np.dot(Xtest - mean, inv(covmats[j])), Xtest - mean)], axis=1))\n",
    "\n",
    "        result_p2[:, j] = Prob\n",
    "        j = j + 1\n",
    "\n",
    "    ypred = np.argmax(result_p2, axis=1) + 1\n",
    "    error = ypred - ytest.ravel()\n",
    "    acc = (Xtest.shape[0] - float(len(np.nonzero(error)[0]))) / Xtest.shape[0]\n",
    "    \n",
    "    return acc,ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnOLERegression(X,y):\n",
    "    # Inputs:                                                         \n",
    "    # X = N x d \n",
    "    # y = N x 1                                                               \n",
    "    # Output: \n",
    "    # w = d x 1 \n",
    "\t\n",
    "    # IMPLEMENT THIS METHOD\n",
    "    \n",
    "    w = np.dot(np.dot(inv(np.dot(X.transpose(), X)), X.transpose()), y)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnRidgeRegression(X,y,lambd):\n",
    "    # Inputs:\n",
    "    # X = N x d                                                               \n",
    "    # y = N x 1 \n",
    "    # lambd = ridge parameter (scalar)\n",
    "    # Output:                                                                  \n",
    "    # w = d x 1                                                                \n",
    "\n",
    "    # IMPLEMENT THIS METHOD \n",
    "#     k = inv(lambd * np.identity(X.shape[1]) + np.dot(X.transpose(), X))\n",
    "#     w = np.dot(np.dot(k, X.transpose()), y)\n",
    "    \n",
    "    Xt_X = np.dot(np.transpose(X), X)\n",
    "    LI = lambd*np.eye(Xt_X.shape[0])\n",
    "    w = np.dot(inv(Xt_X+LI), np.dot(np.transpose(X), y))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testOLERegression(w,Xtest,ytest):\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # Xtest = N x d\n",
    "    # ytest = X x 1\n",
    "    # Output:\n",
    "    # mse\n",
    "    \n",
    "    # IMPLEMENT THIS METHOD\n",
    "    y_predict = np.dot(Xtest,w)\n",
    "    error = ytest - y_predict\n",
    "    mse = np.sum(np.dot(error.transpose(), error)) / float(Xtest.shape[0])\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionObjVal(w, X, y, lambd):\n",
    "\n",
    "    # compute squared error (scalar) and gradient of squared error with respect\n",
    "    # to w (vector) for the given data X and y and the regularization parameter\n",
    "    # lambda                                                                  \n",
    "\n",
    "    # IMPLEMENT THIS METHOD\n",
    "    \n",
    "#     extra_term = y - np.dot(X, np.mat(w).transpose())\n",
    "#     error = 0.5 * np.dot(extra_term.transpose(), extra_term) + 0.5 * lambd * np.dot(np.mat(w).transpose(), np.mat(w))\n",
    "#     error_grad = np.squeeze(np.array(np.dot(np.dot(inv((np.dot(X.transpose(), X) + lambd)), X.transpose()), y)))\n",
    "    \n",
    "    d = X.shape[1]\n",
    "    w_mat = np.reshape(w,(d,1))\n",
    "    inter = y - np.dot(X_i,w_mat)\n",
    "    \n",
    "    error = 0.5*(np.dot(inter.transpose(),inter) + lambd*np.dot(w_mat.transpose(),w_mat))\n",
    "    \n",
    "    # diff the same \n",
    "    # -0.5 * xT (y-Xw) + lambda * w\n",
    "    error_grad = -(np.dot(X_i.transpose(),inter)) + lambd*w_mat\n",
    "    error_grad = np.squeeze(np.array(error_grad))\n",
    "    \n",
    "    return error, error_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-10-bd17e112a1e6>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-bd17e112a1e6>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    Xp[:,column] = pow(x,column)\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def mapNonLinear(x,p):\n",
    "    # Inputs:                                                                  \n",
    "    # x - a single column vector (N x 1)                                       \n",
    "    # p - integer (>= 0)                                                       \n",
    "    # Outputs:                                                                 \n",
    "    # Xp - (N x (p+1)) \n",
    "\t\n",
    "    # IMPLEMENT THIS METHOD\n",
    "    \n",
    "#     Xp_ = []\n",
    "#     for i in range(0, p+1):\n",
    "#         Xp_.append(x**i)\n",
    "#         Xp = np.mat(Xp_)\n",
    "    \n",
    "    N  = x.shape[0]\n",
    "    d  = p + 1\n",
    "    Xp = np.zeros((N,d))\n",
    "    # Iterate per colum and take power of x  with the index. yielding 1,x,x^2,x^3... x^p\n",
    "    for column in range(p+1):\n",
    "        Xp[:,column] = pow(x,column)\n",
    "    return Xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script\n",
    "\n",
    "# Problem 1\n",
    "# load the sample data                                                                 \n",
    "if sys.version_info.major == 2:\n",
    "    X,y,Xtest,ytest = pickle.load(open('sample.pickle','rb'))\n",
    "else:\n",
    "    X,y,Xtest,ytest = pickle.load(open('sample.pickle','rb'),encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "means,covmat = ldaLearn(X,y)\n",
    "ldaacc,ldares = ldaTest(means,covmat,Xtest,ytest)\n",
    "print('LDA Accuracy = '+str(ldaacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QDA\n",
    "means,covmats = qdaLearn(X,y)\n",
    "qdaacc,qdares = qdaTest(means,covmats,Xtest,ytest)\n",
    "print('QDA Accuracy = '+str(qdaacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting boundaries\n",
    "x1 = np.linspace(-5,20,100)\n",
    "x2 = np.linspace(-5,20,100)\n",
    "xx1,xx2 = np.meshgrid(x1,x2)\n",
    "xx = np.zeros((x1.shape[0]*x2.shape[0],2))\n",
    "xx[:,0] = xx1.ravel()\n",
    "xx[:,1] = xx2.ravel()\n",
    "\n",
    "fig = plt.figure(figsize=[12,6])\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "zacc,zldares = ldaTest(means,covmat,xx,np.zeros((xx.shape[0],1)))\n",
    "plt.contourf(x1,x2,zldares.reshape((x1.shape[0],x2.shape[0])),alpha=0.3)\n",
    "plt.scatter(Xtest[:,0], Xtest[:,1],c=ytest[:,0])\n",
    "plt.title('LDA')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "zacc,zqdares = qdaTest(means,covmats,xx,np.zeros((xx.shape[0],1)))\n",
    "plt.contourf(x1,x2,zqdares.reshape((x1.shape[0],x2.shape[0])),alpha=0.3)\n",
    "plt.scatter(Xtest[:,0],Xtest[:,1],c=ytest[:,0])\n",
    "plt.title('QDA')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2\n",
    "if sys.version_info.major == 2:\n",
    "    X,y,Xtest,ytest = pickle.load(open('diabetes.pickle','rb'))\n",
    "else:\n",
    "    X,y,Xtest,ytest = pickle.load(open('diabetes.pickle','rb'),encoding = 'latin1')\n",
    "\n",
    "# add intercept\n",
    "X_i = np.concatenate((np.ones((X.shape[0],1)), X), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "w = learnOLERegression(X,y)\n",
    "mle = testOLERegression(w,Xtest,ytest)\n",
    "\n",
    "w_i = learnOLERegression(X_i,y)\n",
    "mle_i = testOLERegression(w_i,Xtest_i,ytest)\n",
    "\n",
    "print('MSE without intercept '+str(mle))\n",
    "print('MSE with intercept '+str(mle_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3\n",
    "k = 101\n",
    "lambdas = np.linspace(0, 1, num=k)\n",
    "i = 0\n",
    "mses3_train = np.zeros((k,1))\n",
    "mses3 = np.zeros((k,1))\n",
    "for lambd in lambdas:\n",
    "    w_l = learnRidgeRegression(X_i,y,lambd)\n",
    "    mses3_train[i] = testOLERegression(w_l,X_i,y)\n",
    "    mses3[i] = testOLERegression(w_l,Xtest_i,ytest)\n",
    "    i = i + 1\n",
    "fig = plt.figure(figsize=[12,6])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(lambdas,mses3_train)\n",
    "plt.title('MSE for Train Data')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lambdas,mses3)\n",
    "plt.title('MSE for Test Data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 4\n",
    "k = 101\n",
    "lambdas = np.linspace(0, 1, num=k)\n",
    "i = 0\n",
    "mses4_train = np.zeros((k,1))\n",
    "mses4 = np.zeros((k,1))\n",
    "opts = {'maxiter' : 20}    # Preferred value.                                                \n",
    "w_init = np.ones((X_i.shape[1],1))\n",
    "for lambd in lambdas:\n",
    "    args = (X_i, y, lambd)\n",
    "    w_l = minimize(regressionObjVal, w_init, jac=True, args=args,method='CG', options=opts)\n",
    "    w_l = np.transpose(np.array(w_l.x))\n",
    "    w_l = np.reshape(w_l,[len(w_l),1])\n",
    "    mses4_train[i] = testOLERegression(w_l,X_i,y)\n",
    "    mses4[i] = testOLERegression(w_l,Xtest_i,ytest)\n",
    "    i = i + 1\n",
    "fig = plt.figure(figsize=[12,6])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(lambdas,mses4_train)\n",
    "plt.plot(lambdas,mses3_train)\n",
    "plt.title('MSE for Train Data')\n",
    "plt.legend(['Using scipy.minimize','Direct minimization'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lambdas,mses4)\n",
    "plt.plot(lambdas,mses3)\n",
    "plt.title('MSE for Test Data')\n",
    "plt.legend(['Using scipy.minimize','Direct minimization'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 5\n",
    "pmax = 7\n",
    "lambda_opt = 0 # REPLACE THIS WITH lambda_opt estimated from Problem 3\n",
    "mses5_train = np.zeros((pmax,2))\n",
    "mses5 = np.zeros((pmax,2))\n",
    "for p in range(pmax):\n",
    "    Xd = mapNonLinear(X[:,2],p)\n",
    "    Xdtest = mapNonLinear(Xtest[:,2],p)\n",
    "    w_d1 = learnRidgeRegression(Xd,y,0)\n",
    "    mses5_train[p,0] = testOLERegression(w_d1,Xd,y)\n",
    "    mses5[p,0] = testOLERegression(w_d1,Xdtest,ytest)\n",
    "    w_d2 = learnRidgeRegression(Xd,y,lambda_opt)\n",
    "    mses5_train[p,1] = testOLERegression(w_d2,Xd,y)\n",
    "    mses5[p,1] = testOLERegression(w_d2,Xdtest,ytest)\n",
    "\n",
    "fig = plt.figure(figsize=[12,6])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(pmax),mses5_train)\n",
    "plt.title('MSE for Train Data')\n",
    "plt.legend(('No Regularization','Regularization'))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(pmax),mses5)\n",
    "plt.title('MSE for Test Data')\n",
    "plt.legend(('No Regularization','Regularization'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
