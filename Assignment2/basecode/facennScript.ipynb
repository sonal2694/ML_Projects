{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Comparing single layer MLP with deep MLP (using TensorFlow)\n",
    "'''\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.optimize import minimize\n",
    "from scipy.io import loadmat\n",
    "from math import sqrt\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this\n",
    "def initializeWeights(n_in,n_out):\n",
    "    \"\"\"\n",
    "    # initializeWeights return the random weights for Neural Network given the\n",
    "    # number of node in the input layer and output layer\n",
    "\n",
    "    # Input:\n",
    "    # n_in: number of nodes of the input layer\n",
    "    # n_out: number of nodes of the output layer\n",
    "                            \n",
    "    # Output: \n",
    "    # W: matrix of random initial weights with size (n_out x (n_in + 1))\"\"\"\n",
    "    epsilon = sqrt(6) / sqrt(n_in + n_out + 1);\n",
    "    W = (np.random.rand(n_out, n_in + 1)*2* epsilon) - epsilon;\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your sigmoid implementation\n",
    "def sigmoid(z):\n",
    "    sigmoidresult = 1.0 / (1.0 + np.exp(-z))\n",
    "    return sigmoidresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your nnObjFunction implementation\n",
    "def nnObjFunction(params, *args):\n",
    "\n",
    "    n_input, n_hidden, n_class, training_data, training_label, lambdaval = args\n",
    "\n",
    "    w1 = params[0:n_hidden * (n_input + 1)].reshape((n_hidden, (n_input + 1)))\n",
    "    w2 = params[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))\n",
    "    obj_val = 0\n",
    "\n",
    "    # Your code here\n",
    "    \n",
    "    label = np.array(training_label);\n",
    "    rows = label.shape[0];\n",
    "    rowsIndex = np.array([i for i in range(rows)])\n",
    "    training_label = np.zeros((rows,2))\n",
    "    # Set the kth column in \"training_label\" to 1 for label k\n",
    "    training_label[rowsIndex,label.astype(int)]=1\n",
    "\n",
    "    # Adding bias to training data and feed forwarding\n",
    "    BiasTerm = np.ones(training_data.shape[0])\n",
    "    training_data = np.column_stack((training_data,BiasTerm))\n",
    "    num_samples = training_data.shape[0]\n",
    "    \n",
    "    # Finding the hidden output using sigmoid\n",
    "    HiddenOutput = sigmoid(np.dot(training_data,w1.T))\n",
    "    \n",
    "    # Adding bias term to hidden layer\n",
    "    NewBias = np.ones(HiddenOutput.shape[0])\n",
    "    HiddenOutputWithBias = np.column_stack((HiddenOutput, NewBias))\n",
    "\n",
    "    # Finding the final output using sigmoid\n",
    "    FinalOutput = sigmoid(np.dot(HiddenOutputWithBias,w2.T))\n",
    "    \n",
    "    # Calculating error to find the Gradient using formula given in handout\n",
    "    Delta = FinalOutput - training_label\n",
    "    \n",
    "    # Using the formula shared in handout. \n",
    "    Gradient_w2 = np.dot(Delta.T,HiddenOutputWithBias)\n",
    "    Gradient_w1 = np.dot(((1-HiddenOutputWithBias)*HiddenOutputWithBias* (np.dot(Delta,w2))).T,training_data)\n",
    "    \n",
    "    # Updating gradient w1 to remove bias hidden nodes\n",
    "    Gradient_w1 = np.delete(Gradient_w1, n_hidden,0)\n",
    "    \n",
    "    # Calculating NLL error function and gradient of error function\n",
    "    lnFinal = np.log(FinalOutput)\n",
    "    lnOneFinal  = np.log(1-FinalOutput)\n",
    "    part_1 = (np.sum(-1*(training_label*lnFinal+(1 - training_label)*lnOneFinal)))\n",
    "    part_1 = part_1/num_samples\n",
    "    \n",
    "    # Adding Regularization \n",
    "    sw1 = np.sum(np.square(w1))\n",
    "    sw2 = np.sum(np.square(w2))\n",
    "    part_2 = (lambdaval/(2*num_samples))* (sw1 +  sw2)\n",
    "    \n",
    "    # Final formula\n",
    "    obj_val = part_1 + part_2\n",
    "\n",
    "    # Regularization will not impact for lambdaval 0, for others it will\n",
    "    Gradient_w1 = Gradient_w1 + lambdaval * w1\n",
    "    Gradient_w2 = Gradient_w2 + lambdaval * w2  \n",
    "\n",
    "    # Make sure you reshape the gradient matrices to a 1D array. for instance if your gradient matrices are grad_w1 and grad_w2\n",
    "    # you would use code similar to the one below to create a flat array\n",
    "    # obj_grad = np.concatenate((grad_w1.flatten(), grad_w2.flatten()),0)\n",
    "    obj_grad = np.array([])\n",
    "    obj_grad = np.concatenate((Gradient_w1.flatten(), Gradient_w2.flatten()),0)\n",
    "    obj_grad = obj_grad/num_samples\n",
    "    \n",
    "    return (obj_val, obj_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your nnPredict implementation\n",
    "def nnPredict(w1,w2,data):\n",
    "    Num_of_Items=data.shape[0]    \n",
    "\n",
    "    # Adding bias term\n",
    "    Bias = np.zeros([len(data), 1])\n",
    "    DataWithBias = np.append(data, Bias ,1)\n",
    "    \n",
    "    hidden_input = np.dot(DataWithBias ,w1.T)\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "    \n",
    "    # Second layer - Adding Bias Term   \n",
    "    Bias = np.zeros([len(hidden_output), 1])\n",
    "    FinalDataWithBias = np.append(hidden_output, Bias, 1)\n",
    "    final_input = np.dot(FinalDataWithBias, w2.T)\n",
    "    final_output = sigmoid(final_input)\n",
    "\n",
    "    #Initialize an dummy output array\n",
    "    label_list = [-1]*Num_of_Items\n",
    "    for i in range(Num_of_Items):\n",
    "        label_list[i] = np.argmax(final_output[i]);\n",
    "    labels = np.array(label_list)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this\n",
    "def preprocess():\n",
    "    pickle_obj = pickle.load(file=open('face_all.pickle', 'rb'))\n",
    "    features = pickle_obj['Features']\n",
    "    labels = pickle_obj['Labels']\n",
    "    train_x = features[0:21100] / 255\n",
    "    valid_x = features[21100:23765] / 255\n",
    "    test_x = features[23765:] / 255\n",
    "\n",
    "    labels = labels[0]\n",
    "    train_y = labels[0:21100]\n",
    "    valid_y = labels[21100:23765]\n",
    "    test_y = labels[23765:]\n",
    "    return train_x, train_y, valid_x, valid_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"**************Neural Network Script Starts here********************************\"\"\"\n",
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
    "#  Train Neural Network\n",
    "# set the number of nodes in input unit (not including bias unit)\n",
    "n_input = train_data.shape[1]\n",
    "# set the number of nodes in hidden unit (not including bias unit)\n",
    "n_hidden = 256\n",
    "# set the number of nodes in output unit\n",
    "n_class = 2\n",
    "\n",
    "start = time.clock()\n",
    "# initialize the weights into some random matrices\n",
    "initial_w1 = initializeWeights(n_input, n_hidden);\n",
    "initial_w2 = initializeWeights(n_hidden, n_class);\n",
    "# unroll 2 weight matrices into single column vector\n",
    "initialWeights = np.concatenate((initial_w1.flatten(), initial_w2.flatten()),0)\n",
    "# set the regularization hyper-parameter\n",
    "lambdaval = 10;\n",
    "args = (n_input, n_hidden, n_class, train_data, train_label, lambdaval)\n",
    "\n",
    "#Train Neural Network using fmin_cg or minimize from scipy,optimize module. Check documentation for a working example\n",
    "opts = {'maxiter' :50}    # Preferred value.\n",
    "\n",
    "nn_params = minimize(nnObjFunction, initialWeights, jac=True, args=args,method='CG', options=opts)\n",
    "params = nn_params.get('x')\n",
    "#Reshape nnParams from 1D vector into w1 and w2 matrices\n",
    "w1 = params[0:n_hidden * (n_input + 1)].reshape( (n_hidden, (n_input + 1)))\n",
    "w2 = params[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))\n",
    "\n",
    "#Test the computed parameters\n",
    "predicted_label = nnPredict(w1,w2,train_data)\n",
    "#find the accuracy on Training Dataset\n",
    "print('\\n Training set Accuracy:' + str(100*np.mean((predicted_label == train_label).astype(float))) + '%')\n",
    "predicted_label = nnPredict(w1,w2,validation_data)\n",
    "#find the accuracy on Validation Dataset\n",
    "print('\\n Validation set Accuracy:' + str(100*np.mean((predicted_label == validation_label).astype(float))) + '%')\n",
    "predicted_label = nnPredict(w1,w2,test_data)\n",
    "#find the accuracy on Validation Dataset\n",
    "print('\\n Test set Accuracy:' +  str(100*np.mean((predicted_label == test_label).astype(float))) + '%')\n",
    "\n",
    "elapsed = (time.clock() - start)\n",
    "print(\"Time used:\",elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
